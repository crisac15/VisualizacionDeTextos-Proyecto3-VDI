knitr::opts_chunk$set(echo = TRUE)
# Librerías necesarias
install.packages("stopwords")
library(stopwords)
library(tidyverse)
library(ggwordcloud)
library(syuzhet)
library(tm)
library(SnowballC)
library(igraph)
library(ggraph)
library(tokenizers)
library(tidytext)
library(ggplot2)
# Versión 1: texto limpio para wordcloud y red (SIN puntuación)
texto <- readLines("Conversacion con una momia.txt", encoding = "UTF-8")
texto <- tolower(paste(texto, collapse = " "))
texto <- gsub("[[:punct:]]", "", texto)
texto <- gsub("[0-9]", "", texto)
# Versión 2: texto crudo para análisis narrativo (CON puntuación)
texto_narrativo <- readLines("Conversacion con una momia.txt", encoding = "UTF-8")
texto_narrativo <- paste(texto_narrativo, collapse = " ")
datos <- tibble(texto = texto) %>%
unnest_tokens(palabra, texto) %>%
anti_join(stop_words %>% filter(lexicon == "snowball"), by = c("palabra" = "word")) %>%
filter(nchar(palabra) > 4) %>%
count(palabra, name = "frecuencia") %>%
arrange(desc(frecuencia))
ggplot(datos, aes(
label = palabra,
size = frecuencia,
color = frecuencia
)) +
geom_text_wordcloud_area(shape = "circle", rm_outside = TRUE) +
scale_size_area(max_size = 12) +
scale_color_gradient(low = "#1a9641", high = "#d7191c") +
theme_minimal() +
labs(title = "Nube de palabras emocionales")
# 1. Dividir en oraciones
oraciones <- get_sentences(texto_narrativo)
# 2. Obtener emociones NRC en español por oración
emociones <- get_nrc_sentiment(oraciones, lang = "spanish")
# 3. Calcular polaridad: positivo - negativo
polaridad <- emociones$positive - emociones$negative
# 4. Verificar que haya variación emocional
if (length(polaridad) > 2 && any(is.finite(polaridad)) && sd(polaridad) > 0) {
# Aplicar suavizado con DCT
arco <- get_dct_transform(polaridad,
low_pass_size = min(length(polaridad), 20),
scale_range = TRUE)
# 5. Graficar arco emocional
plot(arco, type = "l", col = "darkgreen", lwd = 2,
main = "Arco emocional de 'Conversación con una momia'",
xlab = "Progresión narrativa", ylab = "Polaridad emocional")
abline(h = 0, lty = 2, col = "gray")
} else {
message("No se detectó suficiente polaridad en el texto completo.")
}
datos <- tibble(texto = texto) %>%
unnest_tokens(palabra, texto) %>%
anti_join(stop_words %>% filter(lexicon == "snowball"), by = c("palabra" = "word")) %>%
filter(nchar(palabra) > 4) %>%
count(palabra, name = "frecuencia") %>%
arrange(desc(frecuencia))
ggplot(datos, aes(
label = palabra,
size = frecuencia,
color = frecuencia
)) +
geom_text_wordcloud_area(shape = "circle", rm_outside = TRUE) +
scale_size_area(max_size = 12) +
scale_color_gradient(low = "#1a9641", high = "#d7191c") +
theme_minimal() +
labs(title = "Nube de palabras emocionales")
# Tokenizar oraciones
oraciones <- unlist(tokenize_sentences(texto))
# Definir stopwords completas y personalizadas
stopwords_es <- stopwords("es", source = "snowball")
knitr::opts_chunk$set(echo = TRUE)
# Librerías necesarias
install.packages("stopwords")
library(stopwords)
library(tidyverse)
library(ggwordcloud)
library(syuzhet)
library(tm)
library(SnowballC)
library(igraph)
library(ggraph)
library(tokenizers)
library(tidytext)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
# Librerías necesarias
install.packages("stopwords")
library(stopwords)
library(tidyverse)
library(ggwordcloud)
library(syuzhet)
library(tm)
library(SnowballC)
library(igraph)
library(ggraph)
library(tokenizers)
library(tidytext)
library(ggplot2)
# Tokenizar oraciones
oraciones <- unlist(tokenize_sentences(texto))
# Definir stopwords completas y personalizadas
stopwords_es <- stopwords("es", source = "snowball")
# Paquetes necesarios
if (!require("tm")) install.packages("tm")
if (!require("igraph")) install.packages("igraph")
if (!require("SnowballC")) install.packages("SnowballC")
if (!require("stopwords")) install.packages("stopwords")
if (!require("tokenizers")) install.packages("tokenizers")
library(tm)
library(igraph)
library(SnowballC)
library(stopwords)
library(tokenizers)
# Paso 1: Tokenizar por oraciones
oraciones <- unlist(tokenize_sentences(texto))
# Paso 2: Crear corpus y limpiar
corpus <- VCorpus(VectorSource(oraciones))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("es"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument, language = "spanish")
# Paso 3: Matriz de término-documento
tdm <- TermDocumentMatrix(corpus)
mat <- as.matrix(tdm)
# Paso 4: Filtrar por términos frecuentes
freqs <- rowSums(mat)
top_terms <- names(sort(freqs, decreasing = TRUE))[1:50]
mat <- mat[top_terms, ]
mat[mat >= 1] <- 1  # binarizar
# Paso 5: Matriz de coocurrencia
cooc <- mat %*% t(mat)
diag(cooc) <- 0
cooc[cooc < 2] <- 0  # opcional: filtrar conexiones débiles
# Paso 6: Crear grafo
g <- graph.adjacency(cooc, weighted = TRUE, mode = "undirected")
g <- simplify(g)
g <- delete_vertices(g, degree(g) == 0)
# Paso 7: Visualización
set.seed(123)
layout <- layout_with_fr(g)
plot(g,
layout = layout,
vertex.size = 8,
vertex.label.cex = 0.9,
vertex.label.color = "black",
vertex.color = "lightblue",
edge.width = E(g)$weight,
edge.color = "gray70",
main = "Red de coocurrencia de palabras")
# Paquetes necesarios
if (!require("tm")) install.packages("tm")
if (!require("igraph")) install.packages("igraph")
if (!require("SnowballC")) install.packages("SnowballC")
if (!require("stopwords")) install.packages("stopwords")
if (!require("tokenizers")) install.packages("tokenizers")
library(tm)
library(igraph)
library(SnowballC)
library(stopwords)
library(tokenizers)
# Paso 1: Tokenizar por oraciones
oraciones <- unlist(tokenize_sentences(texto))
# Paso 2: Crear corpus y limpiar
corpus <- VCorpus(VectorSource(oraciones))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("es"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument, language = "spanish")
# Paso 3: Matriz de término-documento
tdm <- TermDocumentMatrix(corpus)
mat <- as.matrix(tdm)
# Paso 4: Filtrar por términos frecuentes
freqs <- rowSums(mat)
top_terms <- names(sort(freqs, decreasing = TRUE))[1:50]
mat <- mat[top_terms, ]
mat[mat >= 1] <- 1  # binarizar
# Paso 5: Matriz de coocurrencia
cooc <- mat %*% t(mat)
diag(cooc) <- 0
cooc[cooc < 2] <- 0  # opcional: filtrar conexiones débiles
# Paso 6: Crear grafo
g <- graph_from_adjacency_matrix(cooc, weighted = TRUE, mode = "undirected")
g <- simplify(g)
g <- delete_vertices(g, degree(g) == 0)
# Paso 7: Visualización
set.seed(123)
layout <- layout_with_fr(g)
plot(g,
layout = layout,
vertex.size = 8,
vertex.label.cex = 0.9,
vertex.label.color = "black",
vertex.color = "lightblue",
edge.width = E(g)$weight,
edge.color = "gray70",
main = "Red de coocurrencia de palabras")
datos <- tibble(texto = texto) %>%
unnest_tokens(palabra, texto) %>%
anti_join(stop_words %>% filter(lexicon == "snowball"), by = c("palabra" = "word")) %>%
filter(nchar(palabra) > 4) %>%
count(palabra, name = "frecuencia") %>%
arrange(desc(frecuencia))
ggplot(datos, aes(
label = palabra,
size = frecuencia,
color = frecuencia
)) +
geom_text_wordcloud_area(shape = "circle", rm_outside = TRUE) +
scale_size_area(max_size = 12) +
scale_color_gradient(low = "#1a9641", high = "#d7191c") +
theme_minimal() +
labs(title = "Nube de palabras emocionales")
gc()
ggplot(datos %>% slice_max(frecuencia, n = 120), aes(
label = palabra,
size = frecuencia,
color = frecuencia
)) +
geom_text_wordcloud_area(shape = "circle", rm_outside = TRUE) +
scale_size_area(max_size = 16) +
scale_color_gradient(low = "#1a9641", high = "#d7191c") +
theme_minimal() +
labs(title = "Nube de palabras emocionales (top 120)")
datos <- tibble(texto = texto) %>%
unnest_tokens(palabra, texto) %>%
anti_join(stop_words %>% filter(lexicon == "snowball"), by = c("palabra" = "word")) %>%
filter(nchar(palabra) > 4) %>%
count(palabra, name = "frecuencia") %>%
arrange(desc(frecuencia))
ggplot(datos, aes(
label = palabra,
size = frecuencia,
color = frecuencia
)) +
geom_text_wordcloud_area(shape = "circle", rm_outside = TRUE) +
scale_size_area(max_size = 12) +
scale_color_gradient(low = "#1a9641", high = "#d7191c") +
theme_minimal() +
labs(title = "Nube de palabras emocionales")
ggplot(datos %>% slice_max(frecuencia, n = 120), aes(
label = palabra,
size = frecuencia,
color = frecuencia
)) +
geom_text_wordcloud_area(shape = "circle", rm_outside = TRUE) +
scale_size_area(max_size = 16) +
scale_color_gradient(low = "#1a9641", high = "#d7191c") +
theme_minimal() +
labs(title = "Nube de palabras emocionales (top 120)")
nrc <- get_nrc_sentiment(get_sentences(texto))
emociones_sum <- colSums(nrc[, 1:8])
barplot(emociones_sum, las = 2, col = rainbow(8), main = "Conteo de emociones NRC")
nrc <- get_nrc_sentiment(get_sentences(texto))
emociones_sum <- colSums(nrc[, 1:8])
barplot(emociones_sum, las = 2, col = rainbow(8), main = "Conteo de emociones NRC")
# Tokenizar en oraciones
oraciones <- unlist(tokenize_sentences(texto))
# Crear corpus y procesar
corpus <- VCorpus(VectorSource(oraciones))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("es"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument, language = "spanish")
# Matriz término-documento
tdm <- TermDocumentMatrix(corpus)
mat <- as.matrix(tdm)
# Seleccionar top 40 palabras más frecuentes
frecuencias <- rowSums(mat)
top_terms <- names(sort(frecuencias, decreasing = TRUE))[1:40]
mat <- mat[top_terms, ]
mat[mat >= 1] <- 1  # binarizar
# Matriz de co-ocurrencia
cooc <- mat %*% t(mat)
diag(cooc) <- 0
cooc[cooc < 2] <- 0  # filtrar conexiones débiles
# Crear grafo
g <- graph.adjacency(cooc, weighted = TRUE, mode = "undirected")
g <- simplify(g)
g <- delete_vertices(g, degree(g) == 0)
# Visualización
set.seed(123)
layout <- layout_with_fr(g)
plot(
g,
layout = layout,
vertex.size = 8,
vertex.label.cex = 0.8,
vertex.label.color = "black",
vertex.color = "#AED6F1",
edge.width = E(g)$weight,
edge.color = "gray60",
main = "Red de palabras más frecuentes"
)
